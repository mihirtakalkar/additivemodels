{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw STL Conversion\n",
    "Data preprocessing for use with MeshGraphNets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trimesh\n",
      "  Downloading trimesh-4.5.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/mihir/miniconda3/lib/python3.12/site-packages (from trimesh) (1.26.4)\n",
      "Downloading trimesh-4.5.2-py3-none-any.whl (704 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.4/704.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trimesh\n",
      "Successfully installed trimesh-4.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as nn\n",
    "import trimesh\n",
    "from trimesh import load as load_stl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stl(file_path):\n",
    "    mesh = trimesh.load(file_path)\n",
    "    vertices = torch.tensor(mesh.vertices, dtype=torch.float)  # Node positions\n",
    "    faces = torch.tensor(mesh.faces, dtype=torch.long)         # Face (triangle) indices\n",
    "    return vertices, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/your/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 85\u001b[0m\n\u001b[1;32m     79\u001b[0m data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/your/data\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path where STL-related CSVs are located\u001b[39;00m\n\u001b[1;32m     80\u001b[0m quality_scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/file1.stl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/file2.stl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# Add paths and corresponding quality scores here\u001b[39;00m\n\u001b[1;32m     84\u001b[0m }\n\u001b[0;32m---> 85\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(data_file, quality_scores)\u001b[0m\n\u001b[1;32m      2\u001b[0m data_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m file_name_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m file_list \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m file_list:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/data'"
     ]
    }
   ],
   "source": [
    "def create_dataset(data_file, quality_scores):\n",
    "    data_list = []\n",
    "    file_name_list = []\n",
    "    file_list = os.listdir(data_file)\n",
    "    j = 1\n",
    "    \n",
    "    for item in file_list:\n",
    "        design_name = item.rpartition('_')[0]\n",
    "        print(f\"Processing file {j}: {design_name}\")\n",
    "        \n",
    "        if design_name not in file_name_list and design_name != \".DS\":\n",
    "            file_name_list.append(design_name)\n",
    "\n",
    "            # Load node positions and elements (faces)\n",
    "            design_node = os.path.join(data_file, f\"{design_name}_nodes.csv\")\n",
    "            design_element = os.path.join(data_file, f\"{design_name}_elements.csv\")\n",
    "\n",
    "            df_nodes = pd.read_csv(design_node)\n",
    "            df_elements = pd.read_csv(design_element)\n",
    "\n",
    "            # Convert four-node 3D elements into triangles\n",
    "            df_triangles = pd.DataFrame(columns=['elem1', 'elem2', 'elem3'])\n",
    "            for i in range(df_elements.shape[0]):\n",
    "                four_nodes = df_elements.iloc[i, -4:].tolist()\n",
    "                triangles = list(itertools.combinations(four_nodes, 3))\n",
    "                df_triangles = df_triangles.append(pd.DataFrame(triangles, columns=['elem1', 'elem2', 'elem3']), ignore_index=True)\n",
    "\n",
    "            # Get cell indices and node positions\n",
    "            cells_index = np.vstack((df_triangles[\"elem1\"].to_numpy(),\n",
    "                                     df_triangles[\"elem2\"].to_numpy(),\n",
    "                                     df_triangles[\"elem3\"].to_numpy())).T.astype('int32')\n",
    "            cells = torch.tensor(cells_index)\n",
    "\n",
    "            node_position = np.vstack((df_nodes[\"x\"].to_numpy(), df_nodes[\"y\"].to_numpy(), df_nodes[\"z\"].to_numpy())).T.astype('float32')\n",
    "            mesh_pos = torch.tensor(node_position)\n",
    "\n",
    "            # Calculate edge indices using triangles_to_edges\n",
    "            edges = triangles_to_edges(cells)\n",
    "            edge_index = torch.cat((torch.tensor(edges[0].numpy()).unsqueeze(0),\n",
    "                                    torch.tensor(edges[1].numpy()).unsqueeze(0)), dim=0).type(torch.long)\n",
    "\n",
    "            # Calculate edge features\n",
    "            u_i = mesh_pos[edge_index[0]]\n",
    "            u_j = mesh_pos[edge_index[1]]\n",
    "            u_ij = u_i - u_j\n",
    "            u_ij_norm = torch.norm(u_ij, p=2, dim=1, keepdim=True)\n",
    "            edge_attr = torch.cat((u_ij, u_ij_norm), dim=-1).type(torch.float)\n",
    "\n",
    "            # Node type and additional features\n",
    "            node_type_info = df_nodes[\"nodetype\"].to_numpy().astype('int32')\n",
    "            node_type = nn.one_hot(torch.tensor(node_type_info).to(torch.long), num_classes=4)  # Assuming 4 types; adjust as needed\n",
    "            x = torch.cat((mesh_pos, node_type), dim=-1).type(torch.float)\n",
    "\n",
    "            # Add quality score as the target\n",
    "            if f\"{design_name}.stl\" in quality_scores:\n",
    "                target = torch.tensor([float(quality_scores[f\"{design_name}.stl\"])], dtype=torch.float)\n",
    "            else:\n",
    "                print(f\"Warning: Quality score not found for {design_name}\")\n",
    "                continue\n",
    "\n",
    "            # Append to data list\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=target, mesh_pos=mesh_pos, cells=cells)\n",
    "            data_list.append(data)\n",
    "\n",
    "        j += 1\n",
    "    print(\"Done collecting data!\")\n",
    "    \n",
    "    # Define the save path\n",
    "    save_dir = os.path.join(data_file, 'data_preprocessed')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, '3D_data_sample.pt')\n",
    "    \n",
    "    # Save dataset\n",
    "    torch.save(data_list, save_path)\n",
    "    print(f\"Dataset saved to: {save_path}\")\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "data_file = \"/path/to/your/data\"  # Path where STL-related CSVs are located\n",
    "quality_scores = {\n",
    "    \"/path/to/file1.stl\": \"1.5\",\n",
    "    \"/path/to/file2.stl\": \"2.0\",\n",
    "    # Add paths and corresponding quality scores here\n",
    "}\n",
    "save_path = create_dataset(data_file, quality_scores)\n",
    "print(f\"Dataset saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
