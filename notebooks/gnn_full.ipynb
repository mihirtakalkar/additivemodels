{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN-based FEA Surrogate Model\n",
    "Experimenting with basic GNN architectures from the repaired dataset leveraging\n",
    "[PyTorch Geometric](https://github.com/pyg-team/pytorch_geometric)\n",
    "\n",
    "Using full dataset with batching.\n",
    "\n",
    "Currently: GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "import glob\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prevent Memory Issues via Lazy Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Class for Loading Graphs from Disk\n",
    "class ProcessedGraphDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(ProcessedGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.processed_files = glob.glob(os.path.join(root, '*.pt'))\n",
    "        \n",
    "        if len(self.processed_files) > 0:\n",
    "            # Load sample graph for props\n",
    "            sample_graph = torch.load(self.processed_files[0])\n",
    "            self._num_node_features = sample_graph.num_node_features\n",
    "            self._num_classes = 1  # Assuming regression\n",
    "        else:\n",
    "            raise ValueError(\"No processed graph files found.\")\n",
    "    \n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return self._num_node_features\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return self._num_classes\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.processed_files)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        graph = torch.load(self.processed_files[idx])\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = ProcessedGraphDataset(root='../data/processed/processed_graphs')\n",
    "\n",
    "# Split\n",
    "train_length = int(len(dataset) * 0.8)\n",
    "test_length = len(dataset) - train_length\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_length, test_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Model (GCNConv)\n",
    "class QualityPredictor(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(QualityPredictor, self).__init__()\n",
    "        # Define layers\n",
    "        self.conv1 = GCNConv(in_channels=num_node_features, out_channels=64)\n",
    "        self.conv2 = GCNConv(in_channels=64, out_channels=32)\n",
    "        self.fc = torch.nn.Linear(in_features=32, out_features=1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply GCN layers with ReLU activation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        # Global mean pooling to get graph-level embeddings\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Output is a single value per graph\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "dataset = ProcessedGraphDataset(root='../data/processed/processed_graphs')\n",
    "\n",
    "# Split the dataset\n",
    "train_length = int(len(dataset) * 0.8)\n",
    "test_length = len(dataset) - train_length\n",
    "train_dataset, test_dataset = random_split(dataset, [train_length, test_length])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 4\n",
    "num_workers = 0 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = QualityPredictor(num_node_features=dataset.num_node_features).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Switch to L1 loss\n",
    "criterion = torch.nn.L1Loss()\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "# Variables for early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "max_patience = 10\n",
    "\n",
    "# Store loss values\n",
    "training_losses = []\n",
    "validation_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Training Loop\n",
    "\n",
    "Time: 472m 36.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch\u001b[38;5;241m.\u001b[39mnum_graphs\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m batch\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     18\u001b[0m training_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/cuda/memory.py:150\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid fraction value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfraction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Allowed range: 0~1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_setMemoryFraction(fraction, device)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempty_cache\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    allocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    `nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m        more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch.y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * batch.num_graphs\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            val_loss = criterion(out, batch.y.view(-1))\n",
    "            total_val_loss += val_loss.item() * batch.num_graphs\n",
    "            del batch\n",
    "            torch.cuda.empty_cache()\n",
    "    avg_val_loss = total_val_loss / len(test_loader.dataset)\n",
    "    validation_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= max_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Epoch 1, Training Loss: 3.2134, Validation Loss: 3.1544\n",
    "Epoch 2, Training Loss: 3.1943, Validation Loss: 3.1501\n",
    "Epoch 3, Training Loss: 3.1926, Validation Loss: 3.1477\n",
    "Epoch 4, Training Loss: 3.1911, Validation Loss: 3.1485\n",
    "Epoch 5, Training Loss: 3.1896, Validation Loss: 3.1479\n",
    "Epoch 6, Training Loss: 3.1888, Validation Loss: 3.1453\n",
    "Epoch 7, Training Loss: 3.1880, Validation Loss: 3.1442\n",
    "Epoch 8, Training Loss: 3.1871, Validation Loss: 3.1458\n",
    "Epoch 9, Training Loss: 3.1866, Validation Loss: 3.1470\n",
    "Epoch 10, Training Loss: 3.1860, Validation Loss: 3.1427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 3.1427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model.eval()\n",
    "true_values = []\n",
    "predicted_values = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        true_values.extend(data.y.cpu().numpy())\n",
    "        predicted_values.extend(output.cpu().numpy())\n",
    "\n",
    "mae = mean_absolute_error(true_values, predicted_values)\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
